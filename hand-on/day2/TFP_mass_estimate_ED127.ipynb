{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFP_mass_estimate_ED127.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhuertascompany/DL_ED127_2021/blob/main/hand-on/day2/TFP_mass_estimate_ED127.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIKPkpeOR3-D"
      },
      "source": [
        "##### Copyright 2019 Matthew Ho, Francois Lanusse.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiP5vuHbSQVq"
      },
      "source": [
        "# Estimating Galaxy Cluster Masses with TensorFlow and TensorFlow Probability\n",
        "\n",
        "This notebook is bases on an example by: \n",
        "  - [@maho3](https://github.com/maho3) (Matt Ho)\n",
        "  - [@EiffL](https://github.com/EiffL) (Francois Lanusse)\n",
        "\n",
        "### Overview\n",
        "\n",
        "In this tutorial, we learn how to combine Keras and TensorFlow Probability to estimate the masses of galaxy clusters using velocity dispersion measurements. See for instance [Ho et al. (2019)](https://arxiv.org/abs/1902.05950) for a state of the art machine learning approach to this problem.\n",
        "\n",
        "The context for this problem is to be able to build robust mass estimates for \n",
        "galaxy clusters, simply by using information from their galaxy members. And in \n",
        "particular line of sight velocity information which can be obtained by spectroscopy.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80MpR9d-RlHu"
      },
      "source": [
        "#@title Imports and Utility functions { display-mode: \"form\" }\n",
        "%pylab inline\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow_probability as tfp\n",
        "tfd = tfp.distributions\n",
        "\n",
        "# Activate TF2 behavior:\n",
        "from tensorflow.python import tf2\n",
        "if not tf2.enabled():\n",
        "  import tensorflow.compat.v2 as tf\n",
        "  tf.enable_v2_behavior()\n",
        "  assert tf2.enabled()\n",
        "\n",
        "def binned_plot(X, Y, n=10, percentiles=[35, 50], ax=None, **kwargs):\n",
        "    # Calculation\n",
        "    calc_percent = []\n",
        "    for p in percentiles:\n",
        "        if p < 50:\n",
        "            calc_percent.append(50-p)\n",
        "            calc_percent.append(50+p)\n",
        "        elif p == 50:\n",
        "            calc_percent.append(50)\n",
        "        else:\n",
        "            raise Exception('Percentile > 50')\n",
        "\n",
        "    bin_edges = np.linspace(X.min()*0.9999, X.max()*1.0001, n+1)\n",
        "\n",
        "    dtype = [(str(i), 'f') for i in calc_percent]\n",
        "    bin_data = np.zeros(shape=(n,), dtype=dtype)\n",
        "\n",
        "    for i in range(n):\n",
        "        y = Y[(X >= bin_edges[i]) & (X < bin_edges[i+1])]\n",
        "\n",
        "        if len(y) == 0:\n",
        "            continue\n",
        "\n",
        "        y_p = np.percentile(y, calc_percent)\n",
        "\n",
        "        bin_data[i] = tuple(y_p)\n",
        "\n",
        "    # Plotting\n",
        "    if ax is None:\n",
        "        f, ax = plt.subplots()\n",
        "\n",
        "    bin_centers = [np.mean(bin_edges[i:i+2]) for i in range(n)]\n",
        "    for p in percentiles:\n",
        "        if p == 50:\n",
        "            ax.plot(bin_centers, bin_data[str(p)], **kwargs)\n",
        "        else:\n",
        "            ax.fill_between(bin_centers,\n",
        "                            bin_data[str(50-p)],\n",
        "                            bin_data[str(50+p)],\n",
        "                            alpha=0.2,\n",
        "                            **kwargs)\n",
        "\n",
        "    return bin_data, bin_edges\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOdxcnGYenbT"
      },
      "source": [
        "#Checking for GPU access\n",
        "if tf.test.gpu_device_name() != '/device:GPU:0':\n",
        "  print('WARNING: GPU device not found.')\n",
        "else:\n",
        "  print('SUCCESS: Found GPU: {}'.format(tf.test.gpu_device_name()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUpk4oK7T8sS"
      },
      "source": [
        "## Understanding the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXqDYLU-cMLw",
        "cellView": "form"
      },
      "source": [
        "#@title Loading dataset.\n",
        "#from google.colab import auth\n",
        "from astropy.table import Table\n",
        "\n",
        "#auth.authenticate_user()\n",
        "bucket_name='ahw2019'\n",
        "\n",
        "!gsutil cp gs://{bucket_name}/halo_mass_regression/'Rockstar_UM_z=0.117_contam_summary.fits' contam_summary.fits\n",
        "\n",
        "print('Download complete')\n",
        "\n",
        "data = Table.read('contam_summary.fits')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0i6vcxqAgfNR"
      },
      "source": [
        "# Let's check the mass distribution in this sample\n",
        "hist(log10(data['M200c']),100)\n",
        "xlabel(r'$\\log[M_{200c}\\ (h^{-1}M_\\odot)]$')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mU6bWyAdOmlp"
      },
      "source": [
        "hist2d(log10(data['M200c']), log10(data['Ngal']),64,cmap='gist_stern');\n",
        "plt.colorbar()\n",
        "xlabel(r'$\\log_{10}[M_{200c}\\ (h^{-1}M_\\odot)]$')\n",
        "ylabel(r'$\\log_{10}[N_\\mathrm{gal}]$')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qA0S4LC58tl"
      },
      "source": [
        "This plot shows the scaling relation between mass and number of cluster members"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KC5oGVC2PNYG"
      },
      "source": [
        "hist2d(log10(data['M200c']), log10(data['sigv']),64,cmap='gist_stern');\n",
        "plt.colorbar()\n",
        "xlabel(r'$\\log_{10}[M_{200c}\\ (h^{-1}M_\\odot)]$')\n",
        "ylabel(r'$\\log_{10}[\\sigma_v\\ (km\\ s^{-1})]$')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egX3KcY16EGI"
      },
      "source": [
        "This plot illustrates the scaling relation between mass and the velocity dispersion of cluster members."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emhjyv2_M-GU"
      },
      "source": [
        "So, we are going to build a regression model, just as the toy examples we just saw, to estimate cluster masses. Instead of doing a regression with only one input parameter, we are going to use 14 input parameters: number of galaxies in the cluster, line of sight veolcity dispersion + some other moments describing the radial distribution of galaxies in the clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oD7CVmu7i3mv"
      },
      "source": [
        "# This cell performs some pre-processing of the input and output features - just run and ignore for now\n",
        "# Preprocessing some features\n",
        "data['LogNgal'] = np.log10(data['Ngal'])\n",
        "data['Logsigv'] = np.log10(data['sigv'])\n",
        "data['logmass'] = np.log10(data['M200c'])\n",
        "\n",
        "# Preparing data set\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "# Split into train and test\n",
        "inds_random = permutation(len(data))\n",
        "data_train = data[inds_random[:200000]]\n",
        "data_test = data[inds_random[200000:]]\n",
        "\n",
        "# Features to use for regression\n",
        "X = data[['LogNgal', # log richness\n",
        "          'Logsigv', # log velocity dispersion\n",
        "          'R_mean', 'R_std', 'R_skew', 'R_kurt', # descriptive features of cluster member projected radius distribution\n",
        "          'm_mean', 'm_std', 'm_skew', 'm_kurt', # \" of member stellar mass distribution\n",
        "          'v_mean', 'v_std', 'v_skew', 'v_kurt'  # \" of member LOS velocity distribution\n",
        "         ]].to_pandas().values\n",
        "\n",
        "X_train = X[inds_random[:200000]]\n",
        "X_test = X[inds_random[200000:]]\n",
        "\n",
        "scaler = MinMaxScaler().fit(data['logmass'].reshape((-1,1)))\n",
        "feature_scaler = StandardScaler().fit(X_train)\n",
        "\n",
        "X_train = feature_scaler.transform(X_train)\n",
        "Y_train = np.clip(scaler.transform(data_train['logmass'].reshape((-1,1))), 1e-5,1-1e-5)\n",
        "\n",
        "X_test = feature_scaler.transform(X_test)\n",
        "Y_test = np.clip(scaler.transform(data_test['logmass'].reshape((-1,1))),1e-5,1-1e-5)\n",
        "\n",
        "Y_true = data_test['logmass']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6UWg6_mUCic"
      },
      "source": [
        "## First approach: Regression\n",
        "\n",
        "We begin by implementing a simple regression model using a mean squared error loss. This is the standard approach taken by most ML papers on this problem.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smwgMzO9UHNE"
      },
      "source": [
        "regression_model = keras.Sequential([\n",
        "    keras.layers.Dense('FILL THIS OUT', activation='relu', input_shape=(14,)),  ## this is your input model\n",
        "    ## FILL OUT THE NEURAL NETWORK HERE\n",
        "    keras.layers.Dense(units=1) ## this is your output - WHAT IS THE ACTIVATION FUNCTION HERE?\n",
        "])\n",
        "\n",
        "pathout='models/mass1'\n",
        "\n",
        "#define callbacks\n",
        "\n",
        "callback = tf.keras.callbacks.LearningRateScheduler(lambda e: 0.001 if e < 2 else 0.0001)  # this is to reduce the LR during training\n",
        "\n",
        "regression_model.compile(optimizer='adam', metrics=[], \"WHAT IS MISSING HERE TO PERFORM A REGRESSION?\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wiu7CnFUIDk"
      },
      "source": [
        "regression_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uq2kAI8Di0Cl"
      },
      "source": [
        "history = regression_model.fit(\"FILL THIS OUT\", epochs=5, batch_size=100, callbacks=[callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjElsG05U6hc"
      },
      "source": [
        "plot(history.history['loss'])\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hF8ZJbhu6zUe"
      },
      "source": [
        "Now that the model is trained, let's extract some predictions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLoZIqRDi0b_"
      },
      "source": [
        "Y_pred = scaler.inverse_transform(regression_model.predict(X_test)).squeeze()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFHjAIMYi0i-"
      },
      "source": [
        "## EXPLORE THE RESULTS HERE:\n",
        "\n",
        "f = plt.figure(figsize=(6,8))\n",
        "gs = mpl.gridspec.GridSpec(2,1,height_ratios=[3,1], hspace=0)\n",
        "\n",
        "ax1 = f.add_subplot(gs[0,0])\n",
        "\n",
        "ax1.plot(np.arange(13,16,0.1),np.arange(13,16,0.1),'k--')\n",
        "_ = binned_plot(Y_true, \n",
        "                Y_pred, \n",
        "                n=20, percentiles=[35,45,50], \n",
        "                color='b', ax=ax1)\n",
        "\n",
        "ax1.set_xlim(13.5,15.3)\n",
        "ax1.set_ylim(13.5,15.3)\n",
        "ax1.set_xticks([])\n",
        "ax1.set_yticks(ax1.get_yticks()[1:])\n",
        "ax1.set_ylabel(r'$\\log_{10}\\left[M_\\mathrm{pred}\\ (M_\\odot h^{-1})\\right]$', fontsize=14)\n",
        "\n",
        "ax2 = f.add_subplot(gs[1,0])\n",
        "\n",
        "ax2.plot(np.arange(13,16,0.1),[0]*30,'k--')\n",
        "_ = binned_plot(Y_true, \n",
        "                Y_pred - Y_true, \n",
        "                n=20, percentiles=[35,45,50], \n",
        "                color='b', ax=ax2)\n",
        "ax2.set_xlim(13.5,15.3)\n",
        "ax2.set_ylim(-0.5,0.5)\n",
        "ax2.set_xlabel(r'$\\log_{10}\\left[M_\\mathrm{true}\\ (M_\\odot h^{-1})\\right]$', fontsize=14)\n",
        "ax2.set_ylabel(r'$\\epsilon$', fontsize=14)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOEhBisG68Kv"
      },
      "source": [
        "We see a significant bias... What could be the cause? What did we do wrong?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-xXGjBQUI4z"
      },
      "source": [
        "## Second approach: Probabilistic Modelling\n",
        "\n",
        "By now we now that using a MSE can lead to biased estimates. How about we switch to a model of the full posterior, that should fix our problems!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jgpOmdJUMaj"
      },
      "source": [
        "num_components = 16\n",
        "event_shape = [1]\n",
        "\n",
        "params_size = tfp.layers.MixtureSameFamily.params_size(\n",
        "    num_components,\n",
        "    component_params_size=tfp.layers.IndependentNormal.params_size(event_shape))\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(units=128, activation='relu', input_shape=(14,)),\n",
        "    ## COPY THE SAME NETWORK HERE\n",
        "    keras.layers.Dense(units=params_size, activation=None),  # THE OUTPUT IS NOW MADE OF DISTRIBUTION...WE USE HERE A BETA DISTRIBUTION: https://en.wikipedia.org/wiki/Beta_distribution\n",
        "    tfp.layers.MixtureSameFamily(num_components, tfp.layers.IndependentNormal(event_shape))\n",
        "])\n",
        "\n",
        "negloglik = lambda y, p_y: -p_y.log_prob(y)  ## THIS DEFINES THE LOSS - CAN YOU MAKE SENSE OF IT?\n",
        "\n",
        "pathout='models/mass2'\n",
        "#define callbacks\n",
        "from keras.callbacks import TensorBoard\n",
        "tensorboard = TensorBoard(log_dir=pathout)\n",
        "callback = tf.keras.callbacks.LearningRateScheduler(lambda e: 0.001 if e < 5 else 0.0001)\n",
        "\n",
        "model.compile(\"WHAT SHOULD I PUT HERE?\", optimizer='adam', metrics=[])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMaqavDFUMwe"
      },
      "source": [
        "history = model.fit(X_train, Y_train, epochs=10, batch_size=100, callbacks=[callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyBuY586as8R"
      },
      "source": [
        "plot(history.history['loss'])\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOJLg6gKmMfL"
      },
      "source": [
        "# Computing mean of the posterior, and output of the previous regression model\n",
        "# for comparison\n",
        "Y_pred = scaler.inverse_transform(model(X_test).mean().numpy()).squeeze()\n",
        "Y_pred_reg = scaler.inverse_transform(regression_model.predict(X_test)).squeeze()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAe6Zora9qd9"
      },
      "source": [
        "# Let's plot some posterior distributions\n",
        "x = linspace(13.51,15.2,100)\n",
        "# This returns the distribution q(M | x) for all clusters\n",
        "outputs = model(X_test)\n",
        "xt = scaler.transform(x.reshape((-1,1)))\n",
        "logps = []\n",
        "\n",
        "for i in range(len(x)):\n",
        "    logps.append(outputs.log_prob(xt[i]).numpy())\n",
        "logps = np.stack(logps)\n",
        "\n",
        "for i in range(10):\n",
        "  figure()\n",
        "  plot(x, np.exp(logps[:,-i]), label='posterior')\n",
        "  axvline(Y_true[-i], color='m', label='True value')\n",
        "  xlabel(r'$\\log_{10}[M_{200c}\\ (h^{-1}M_\\odot)]$')\n",
        "  legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORW15dhAmMaK"
      },
      "source": [
        "\n",
        "# PLOT HERE THE PREDICTIONS OF YOUR PROBABILSIITC MODEL ALONG WITH UNCERTAINTIES"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Xhcw8pLVO6p"
      },
      "source": [
        "### What is my prior?\n",
        "\n",
        "The output of the Mixture Density Network can be seen as a posterior distribution under the prior defined by the distribution of masses in the training set. \n",
        "\n",
        "Let's have a look at this distribution $p(M_{200c})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFr3K0ImVTtW"
      },
      "source": [
        "import scipy.stats\n",
        "hist = np.histogram(scaler.inverse_transform(Y_train), 64)\n",
        "prior = scipy.stats.rv_histogram(hist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Blp9nE-HVUge"
      },
      "source": [
        "plt.hist(scaler.inverse_transform(Y_train), 100, density=True);\n",
        "x = linspace(13.51,15.2,100)\n",
        "plot(x, prior.pdf(x))\n",
        "xlabel(r'$\\log_{10}[M_{200c}\\ (h^{-1}M_\\odot)]$')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rReEJeJZVe8t"
      },
      "source": [
        "So it's definitely not flat, as a matter of fact, it's heavily  preferring lower mass halos. This could explain why our posterior masses are systematically predicted low when we take the mean of the posterior.\n",
        "\n",
        "Let's compute the mass PDF predicted by the model for  all clusters in the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEruNAFXVgsn"
      },
      "source": [
        "# This returns the distribution q(M | x) for all clusters\n",
        "outputs = model(X_test)\n",
        "xt = scaler.transform(x.reshape((-1,1)))\n",
        "logps = []\n",
        "\n",
        "for i in range(len(x)):\n",
        "    logps.append(outputs.log_prob(xt[i]).numpy())\n",
        "logps = np.stack(logps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxBCygJSVi8b"
      },
      "source": [
        "Now that we have the prior and posterior distributions, we can replace the training set prior $\\tilde{p}$ by a flat prior $p$ simply by using the formula:\n",
        "$$ q(M_{200c} | x ) \\propto \\frac{\\tilde{p}(M_{200c})}{p(M_{200c})} p(M_{200c} | x) $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9P_tdWLVlco"
      },
      "source": [
        "for i in range(10):\n",
        "  figure()\n",
        "  plot(x, np.exp(logps[:,-i]), label='posterior under training prior')\n",
        "  plot(x, np.exp(logps[:,-i])/prior.pdf(x), label='posterior under flat prior')\n",
        "  axvline(Y_true[-i], color='m', label='True value')\n",
        "  xlabel(r'$\\log_{10}[M_{200c}\\ (h^{-1}M_\\odot)]$')\n",
        "  legend()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}